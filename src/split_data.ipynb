{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split your data into Train-Test-Validation splits\n",
    "## Store these splits in a CSV file in the splits folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our sample dataset of chestmnist, they have already split it into train, test and validation sets. We'll go ahead and use these splits but let's take note that it will not always be this easy.\n",
    "Often, we have to create the splits ourselves.\n",
    "To do that, I often get all the file names, shuffle them, and then split them into the desired proportions.\n",
    "Then, I write these splits into a CSV file in the splits folder.\n",
    "So, splits/ will contain CSVs of which files from our data pool we'll be using.\n",
    "Doing it this way ensures that we can always recreate the splits if we need to. It also allows us to remove bad/contaminated samples from our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "SPLIT_DIR = '/home/sasank.desaraju/med-start/splits'\n",
    "DATA_SRC = '/home/sasank.desaraju/med-start/data/chestmnist_64.npz'\n",
    "SPLIT_NAME = 'my_split_64'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split as tts\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Create train/test/val CSVs where each one has the following columns:\n",
    "- image_path: path to the image\n",
    "- label: label of the image\n",
    "- split: train/test/val\n",
    "- patient_id: patient id of the image\n",
    "\"\"\"\n",
    "data = np.load(DATA_SRC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(os.path.join(SPLIT_DIR, f'{SPLIT_NAME}')):\n",
    "    os.makedirs(os.path.join(SPLIT_DIR, f'{SPLIT_NAME}'))\n",
    "\n",
    "train_images = data['train_images']\n",
    "train_labels = data['train_labels']\n",
    "\n",
    "with open(os.path.join(SPLIT_DIR, f'{SPLIT_NAME}', f'train_{SPLIT_NAME}.csv'), 'w') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(['image', 'label', 'stage', 'patient_id'])\n",
    "    for i in range(len(train_images)):\n",
    "        #writer.writerow([train_images[i], train_labels[i], 'train', train_images[i].split('/')[-2]])\n",
    "        writer.writerow([i, i, 'train', i])         # This is a bit silly that we're just using the index as the image path, but it's useful when the data is not as nicely organized as the chestmnist data.\n",
    "\n",
    "test_images = data['test_images']\n",
    "test_labels = data['test_labels']\n",
    "\n",
    "with open(os.path.join(SPLIT_DIR, f'{SPLIT_NAME}', f'test_{SPLIT_NAME}.csv'), 'w') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(['image', 'label', 'stage', 'patient_id'])\n",
    "    for i in range(len(test_images)):\n",
    "        #writer.writerow([test_images[i], test_labels[i], 'test', test_images[i].split('/')[-2]])\n",
    "        writer.writerow([i, i, 'test', i])\n",
    "\n",
    "val_images = data['val_images']\n",
    "val_labels = data['val_labels']\n",
    "\n",
    "with open(os.path.join(SPLIT_DIR, f'{SPLIT_NAME}', f'val_{SPLIT_NAME}.csv'), 'w') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(['image', 'label', 'stage', 'patient_id'])\n",
    "    for i in range(len(val_images)):\n",
    "        #writer.writerow([val_images[i], val_labels[i], 'val', val_images[i].split('/')[-2]])\n",
    "        writer.writerow([i, i, 'val', i])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is how to do this if you just have a bunch of train and label files in a folder that you have to split into train, test and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a CSV of all of the images and labels we have\n",
    "\"\"\"\n",
    "Create a master CSV with the image and label names.\n",
    "\"\"\"\n",
    "# Get the list of folders\n",
    "folders = glob.glob(IMAGE_ROOT + '*')\n",
    "# Create the master CSV\n",
    "with open(DATA_ROOT + master_csv_name, 'w') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(['image', 'label', 'patient_id'])\n",
    "    for folder in folders:\n",
    "        # Get the image and label names\n",
    "        image = os.path.join(folder.split('/')[-1], folder.split('/')[-1] + '_image.nii.gz')\n",
    "        label = os.path.join(folder.split('/')[-1], folder.split('/')[-1] + '_label.nii.gz')\n",
    "        patient_id = folder.split('/')[-1]\n",
    "        # Write the image and label names to the CSV\n",
    "        writer.writerow([image, label, patient_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the train/test/val CSVs\n",
    "\"\"\"\n",
    "Create the train/test/val CSVs.\n",
    "\"\"\"\n",
    "# Read in the master CSV\n",
    "df = pd.read_csv(DATA_ROOT + master_csv_name)\n",
    "# Split the data into train/test/val\n",
    "train, test = tts(df, test_size=0.2, random_state=42)\n",
    "train, val = tts(train, test_size=0.2, random_state=42)\n",
    "# Make the subdirectory for the data split using the data_name if it doesn't exist\n",
    "if not os.path.exists(DATA_ROOT + data_name):\n",
    "    os.mkdir(DATA_ROOT + data_name)\n",
    "# Write the CSVs\n",
    "train.to_csv(os.path.join(DATA_ROOT, data_name, 'train_' + data_name + '.csv'), index=False)\n",
    "test.to_csv(os.path.join(DATA_ROOT, data_name, 'test_' + data_name + '.csv'), index=False)\n",
    "val.to_csv(os.path.join(DATA_ROOT, data_name, 'val_' + data_name + '.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/sasank/Documents/GitRepos/Fistula-Segmentation/data/full_data.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 76\u001b[0m\n\u001b[1;32m     71\u001b[0m     val\u001b[38;5;241m.\u001b[39mto_csv(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(DATA_ROOT, data_name, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m data_name \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.csv\u001b[39m\u001b[38;5;124m'\u001b[39m), index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;66;03m# Create the master CSV\u001b[39;00m\n\u001b[0;32m---> 76\u001b[0m     \u001b[43mcreate_master_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmaster_csv_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfull_data.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m     \u001b[38;5;66;03m# Create the train/test/val CSVs\u001b[39;00m\n\u001b[1;32m     78\u001b[0m     create_train_test_val_csvs(data_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBaseSplit\u001b[39m\u001b[38;5;124m'\u001b[39m, master_csv_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfull_data.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[5], line 45\u001b[0m, in \u001b[0;36mcreate_master_csv\u001b[0;34m(master_csv_name)\u001b[0m\n\u001b[1;32m     43\u001b[0m folders \u001b[38;5;241m=\u001b[39m glob\u001b[38;5;241m.\u001b[39mglob(IMAGE_ROOT \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m# Create the master CSV\u001b[39;00m\n\u001b[0;32m---> 45\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mDATA_ROOT\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmaster_csv_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mw\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m     46\u001b[0m     writer \u001b[38;5;241m=\u001b[39m csv\u001b[38;5;241m.\u001b[39mwriter(f)\n\u001b[1;32m     47\u001b[0m     writer\u001b[38;5;241m.\u001b[39mwriterow([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpatient_id\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[0;32m~/.conda/envs/med-start/lib/python3.10/site-packages/IPython/core/interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    322\u001b[0m     )\n\u001b[0;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/sasank/Documents/GitRepos/Fistula-Segmentation/data/full_data.csv'"
     ]
    }
   ],
   "source": [
    "# This is a file I used to do this for another project where I had my images and labels in separate folders and had to create the split myself.\n",
    "\n",
    "\"\"\"\n",
    "Sasank Desaraju\n",
    "4/4/23\n",
    "\n",
    "This is to create a train/test/val split of CSVs for our Fistula Segmentation dataset.\n",
    "\"\"\"\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split as tts\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import csv\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Each pair of image and label are inside of their own folder with a numerical identifier.\n",
    "Within each folder, the image and label are named [number]_image.nii.gz and [number]_label.nii.gz.\n",
    "For example, the image and label for the pair with the identifier 1 are located at:\n",
    "    IMAGE_ROOT/1/1_image.nii.gz\n",
    "    IMAGE_ROOT/1/1_label.nii.gz\n",
    "\n",
    "We want to create, first, a master CSV that just contains a list of the iamges and their labels in two columns.\n",
    "\n",
    "Then, we want to split this into train/test/val sets and create CSVs of each of the splits.\n",
    "The CSVs should be in the format:\n",
    "    image_name, label_name\n",
    "where both the image_name and label are relative to the IMAGE_ROOT.\n",
    "\"\"\"\n",
    "\n",
    "DATA_ROOT = '/home/sasank/Documents/GitRepos/Fistula-Segmentation/data/'\n",
    "IMAGE_ROOT = '/media/sasank/LinuxStorage/Dropbox (UFL)/FistulaData/Segmentations/'\n",
    "\n",
    "def create_master_csv(master_csv_name):\n",
    "    \"\"\"\n",
    "    Create a master CSV with the image and label names.\n",
    "    \"\"\"\n",
    "    # Get the list of folders\n",
    "    folders = glob.glob(IMAGE_ROOT + '*')\n",
    "    # Create the master CSV\n",
    "    with open(DATA_ROOT + master_csv_name, 'w') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(['image', 'label', 'patient_id'])\n",
    "        for folder in folders:\n",
    "            # Get the image and label names\n",
    "            image = os.path.join(folder.split('/')[-1], folder.split('/')[-1] + '_image.nii.gz')\n",
    "            label = os.path.join(folder.split('/')[-1], folder.split('/')[-1] + '_label.nii.gz')\n",
    "            patient_id = folder.split('/')[-1]\n",
    "            # Write the image and label names to the CSV\n",
    "            writer.writerow([image, label, patient_id])\n",
    "\n",
    "def create_train_test_val_csvs(data_name, master_csv_name):\n",
    "    \"\"\"\n",
    "    Create the train/test/val CSVs.\n",
    "    \"\"\"\n",
    "    # Read in the master CSV\n",
    "    df = pd.read_csv(DATA_ROOT + master_csv_name)\n",
    "    # Split the data into train/test/val\n",
    "    train, test = tts(df, test_size=0.2, random_state=42)\n",
    "    train, val = tts(train, test_size=0.2, random_state=42)\n",
    "    # Make the subdirectory for the data split using the data_name if it doesn't exist\n",
    "    if not os.path.exists(DATA_ROOT + data_name):\n",
    "        os.mkdir(DATA_ROOT + data_name)\n",
    "    # Write the CSVs\n",
    "    train.to_csv(os.path.join(DATA_ROOT, data_name, 'train_' + data_name + '.csv'), index=False)\n",
    "    test.to_csv(os.path.join(DATA_ROOT, data_name, 'test_' + data_name + '.csv'), index=False)\n",
    "    val.to_csv(os.path.join(DATA_ROOT, data_name, 'val_' + data_name + '.csv'), index=False)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Create the master CSV\n",
    "    create_master_csv(master_csv_name='full_data.csv')\n",
    "    # Create the train/test/val CSVs\n",
    "    create_train_test_val_csvs(data_name='BaseSplit', master_csv_name='full_data.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "med-start",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
